# activemq-high-throughput
activemq with high throughput for testing

![executionprocess](./activemq%20execution%20report%20in%20batches.png)

![performance](./activemq%20highthroughput%20.png)
here recheck pending 


*** itâ€™s **high-throughput + no-loss + scalable**:

---

### âœ… **Producer side (GpsPublisher + JmsTemplate)**

* `jmsTemplate.setDeliveryPersistent(true)` â†’ ensures **no data loss** (messages written to broker store).
* `connectionFactory.setUseAsyncSend(true)` + `setAlwaysSyncSend(false)` â†’ allows **asynchronous sends**, much faster than sync.
* `setCopyMessageOnSend(false)` â†’ reduces GC overhead.
* Bulk publishing loop is fine â€” you can even parallelize with `ExecutorService` if you need *10k+/sec* from one JVM.

âœ”ï¸ Producer is optimized for speed + persistence.

---

### âœ… **Consumer side (GpsSubscriber + Listener factory)**

* `DefaultJmsListenerContainerFactory` with `concurrency="50-100"` â†’ allows **50â€“100 parallel consumers per JVM instance**.
* Using **Queue** mode (`factory.setPubSubDomain(false)`) â†’ enables **real load balancing** across consumers.
* Logging: replaced `System.out` with `Slf4j async logging` â†’ avoids console bottlenecks.
* Rate calculation every 5s â†’ smart, avoids per-message log spam.

âœ”ï¸ Consumer is optimized for parallel consumption + logging efficiency.

---

### âœ… **Broker configuration**

* ActiveMQ prefetch set: `factory.getPrefetchPolicy().setQueuePrefetch(1000)` â†’ helps throughput.
* `producerWindowSize=1024000` â†’ batching for producers.
* Persistent delivery mode ensures **no data loss**, but bottleneck will be **disk I/O**.

ğŸ‘‰ For very high throughput (millions/sec), consider:

* Put ActiveMQ on **SSD disks**.
* Tune `systemUsage` in `activemq.xml` (`memoryLimit`, `storeUsage`).
* If you go beyond \~50k/sec, consider **ActiveMQ Artemis** (faster engine) or **Kafka**.

---

### âœ… **Logging**

* `logback-spring.xml` with **async file appender** â†’ âœ… correct.
* `queueSize=10000`, `discardingThreshold=0` â†’ ensures **no log drops**.
* Rolling policy keeps 7 days â†’ production-friendly.

âœ”ï¸ Logging wonâ€™t block your app now.

---

### ğŸš€ Final verdict

* **Producer** â†’ fast, async, persistent.
* **Consumer** â†’ concurrent, load-balanced, async logging.
* **Broker** â†’ properly tuned for high throughput.
* **Logging** â†’ no bottleneck.

ğŸ‘‰ Yes, this setup can handle **huge bulk per second** (10k+/sec depending on hardware & disk).
ğŸ‘‰ **No data loss** (thanks to persistent delivery).
ğŸ‘‰ **Fast throughput** (async send + batching + concurrent consumers).

---


---
---

Q) DefaultJmsListenerContainerFactory with concurrency="50-100" â†’ allows 50â€“100 parallel consumers per JVM instance if i pass 500-1000 then also it perform well or not with no data loss?

---

### âš¡ `concurrency="500-1000"` â€” what happens?

When you configure:

```java
factory.setConcurrency("500-1000");
```

you are telling Spring JMS to spin up **up to 1000 consumer threads in *this JVM*** for that queue.

* âœ… **No data loss** â†’ JMS guarantees delivery (messages will remain in broker store until acknowledged).
* âŒ **But performance may *degrade*** beyond a point:

  * **Thread contention** â†’ 500â€“1000 threads in a single JVM = massive context switching, GC overhead.
  * **Broker load** â†’ ActiveMQ must manage 1000 active consumers â†’ increases network chatter (ACKs, prefetch distribution).
  * **Memory pressure** â†’ each consumer prefetches messages (you set prefetch=1000).

    * With 1000 consumers Ã— 1000 prefetch = **1M messages sitting in consumersâ€™ memory**, possibly stuck.

---

### âš ï¸ Why 50â€“100 is usually the sweet spot

* 50â€“100 consumers per JVM â†’ enough parallelism to saturate CPU and network.
* Going to 500+ consumers in a single JVM **does not scale linearly** â€” instead it creates bottlenecks in threads, broker dispatch, and GC.

---

### ğŸš€ How to really scale beyond that

If you need **>1000 parallel consumers** safely:

1. **Multiple JVM instances (microservices)**:

   * Run 10 JVMs, each with `concurrency=100`.
   * ActiveMQ Queue load balances across them automatically.
   * Much cleaner than `concurrency=1000` in one JVM.

2. **Horizontal scale consumers** â†’ deploy across multiple servers (e.g., Kubernetes, Docker).

3. **Broker-side tuning**:

   * Adjust `systemUsage` (memory/disk limits).
   * Increase `transportConnector` thread pools.
   * Tune `prefetchPolicy` (e.g., `50` instead of `1000`) to avoid one consumer hoarding messages.

---

### âœ… Answer

* If you set `500-1000`, **it will not lose data** (JMS persistence guarantees).
* But **performance will probably get worse**, not better.
* Best practice:

  * Keep `50â€“100` concurrency per JVM.
  * Scale horizontally (more instances) instead of stuffing thousands of threads into one process.

---

Q) if jvm run 10 then how much it can handle?

Good one ğŸ‘ â€” letâ€™s calculate realistically how much **10 JVMs** can handle.

---

### ğŸ”¹ Current setup recap

* You configured:

  * `concurrency="50-100"` â†’ \~100 consumers per JVM.
  * `prefetch=1000` â†’ each consumer can hold up to 1000 messages in memory.
  * Optimizations (async send, optimize acknowledge, batching, async logging) â†’ reduce overhead.

---

### ğŸ”¹ Throughput math

#### 1. One consumer speed

* A **single consumer** can typically process **2kâ€“10k msgs/sec** depending on:

  * message size (\~1KB is easy, \~256KB will be slower).
  * processing logic (just counting/logging = fast, DB writes = slower).
  * hardware (CPU/memory/network).

Letâ€™s assume **5k msgs/sec per consumer** as a balanced estimate.

#### 2. One JVM with \~100 consumers

```
100 consumers Ã— 5k msgs/sec = ~500k msgs/sec (theoretical max)
```

âš ï¸ In practice, due to thread contention, context switches, and broker round-robin limits, youâ€™ll get more like **100kâ€“200k msgs/sec per JVM** stable.

#### 3. 10 JVMs (horizontally scaled)

```
10 JVMs Ã— 100kâ€“200k msgs/sec = ~1Mâ€“2M msgs/sec
```

So with **10 JVMs, each with \~100 consumers**, your system could stably consume **over 1 million messages per second** (assuming message size is small, broker is tuned, and network isnâ€™t the bottleneck).

---

### ğŸ”¹ Bottlenecks to watch

1. **Broker (ActiveMQ Classic)**

   * Default broker isnâ€™t designed for sustained 1M/sec.
   * You must increase `systemUsage` limits, persistence adapter (KahaDB) tuning, and possibly cluster brokers.

2. **Message size**

   * 1KB message â†’ fine at millions/sec.
   * 100KB message â†’ throughput drops drastically (network & disk I/O bottleneck).

3. **Consumer processing**

   * If consumers write to DB â†’ DB becomes the bottleneck.
   * For raw in-memory consumption â†’ you can push millions/sec.

---

### âœ… Answer

If you run **10 JVMs with \~100 consumers each**, you can realistically consume **\~1M messages/sec** (with small messages, lightweight processing, and a tuned broker).

* Data will not be lost (persistent queue + auto-ack).
* Scale further by adding **more JVMs or brokers**.

---
