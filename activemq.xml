<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="
        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
        http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <!-- Properties -->
    <bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
        <property name="locations">
            <value>file:${activemq.conf}/credentials.properties</value>
        </property>
    </bean>

    <!-- Broker configuration -->
    <broker xmlns="http://activemq.apache.org/schema/core" brokerName="localhost" dataDirectory="${activemq.data}">

        <!-- Destination policy -->
        <destinationPolicy>
            <policyMap>
                <policyEntries>
                    <policyEntry queue=">" producerFlowControl="false" memoryLimit="4gb">
                        <!-- Limits messages retained per slow consumer -->
                        <pendingMessageLimitStrategy>
                            <constantPendingMessageLimitStrategy limit="5000"/>
                        </pendingMessageLimitStrategy>
                    </policyEntry>
                </policyEntries>
            </policyMap>
        </destinationPolicy>

        <!-- Persistence adapter: tuned KahaDB -->
        <persistenceAdapter>
            <kahaDB directory="${activemq.data}/kahadb"
                    journalMaxFileLength="128mb"
                    enableIndexWriteAsync="true"
                    enableJournalDiskSyncs="true"
                    checkpointInterval="3000"/>
        </persistenceAdapter>

        <!-- System usage limits -->
        <systemUsage>
            <memoryUsage>
                <memoryUsage limit="16 gb"/>
            </memoryUsage>
            <storeUsage>
                <storeUsage limit="200 gb"/>
            </storeUsage>
            <tempUsage>
                <tempUsage limit="100 gb"/>
            </tempUsage>
        </systemUsage>

        <!-- Transport connectors -->
        <transportConnectors>
            <transportConnector name="openwire" uri="tcp://0.0.0.0:61616?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="amqp" uri="amqp://0.0.0.0:5672?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="stomp" uri="stomp://0.0.0.0:61613?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="mqtt" uri="mqtt://0.0.0.0:1883?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="ws" uri="ws://0.0.0.0:61614?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
        </transportConnectors>

        <!-- JMX management (optional) -->
        <managementContext>
            <managementContext createConnector="false"/>
        </managementContext>

        <!-- Shutdown hooks -->
        <shutdownHooks>
            <bean class="org.apache.activemq.hooks.SpringContextHook"/>
        </shutdownHooks>

    </broker>

    <!-- Web console -->
    <import resource="jetty.xml"/>

</beans>


---
---
*** Explanation 

---
Perfect! Let‚Äôs create a **high-throughput ActiveMQ Classic `activemq.xml`** tuned for persistent messages, bulk publishing, and horizontal scaling. I‚Äôll provide the full XML **first**, then explain each section.

---

## üîπ **Tuned ActiveMQ Classic Broker XML**

```xml
<beans xmlns="http://www.springframework.org/schema/beans"
       xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xsi:schemaLocation="
        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
        http://activemq.apache.org/schema/core http://activemq.apache.org/schema/core/activemq-core.xsd">

    <!-- Properties -->
    <bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer">
        <property name="locations">
            <value>file:${activemq.conf}/credentials.properties</value>
        </property>
    </bean>

    <!-- Broker configuration -->
    <broker xmlns="http://activemq.apache.org/schema/core" brokerName="localhost" dataDirectory="${activemq.data}">

        <!-- Destination policy -->
        <destinationPolicy>
            <policyMap>
                <policyEntries>
                    <policyEntry queue=">" producerFlowControl="false" memoryLimit="4gb">
                        <!-- Limits messages retained per slow consumer -->
                        <pendingMessageLimitStrategy>
                            <constantPendingMessageLimitStrategy limit="5000"/>
                        </pendingMessageLimitStrategy>
                    </policyEntry>
                </policyEntries>
            </policyMap>
        </destinationPolicy>

        <!-- Persistence adapter: tuned KahaDB -->
        <persistenceAdapter>
            <kahaDB directory="${activemq.data}/kahadb"
                    journalMaxFileLength="128mb"
                    enableIndexWriteAsync="true"
                    enableJournalDiskSyncs="true"
                    checkpointInterval="3000"/>
        </persistenceAdapter>

        <!-- System usage limits -->
        <systemUsage>
            <memoryUsage>
                <memoryUsage limit="16 gb"/>
            </memoryUsage>
            <storeUsage>
                <storeUsage limit="200 gb"/>
            </storeUsage>
            <tempUsage>
                <tempUsage limit="100 gb"/>
            </tempUsage>
        </systemUsage>

        <!-- Transport connectors -->
        <transportConnectors>
            <transportConnector name="openwire" uri="tcp://0.0.0.0:61616?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="amqp" uri="amqp://0.0.0.0:5672?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="stomp" uri="stomp://0.0.0.0:61613?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="mqtt" uri="mqtt://0.0.0.0:1883?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
            <transportConnector name="ws" uri="ws://0.0.0.0:61614?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
        </transportConnectors>

        <!-- JMX management (optional) -->
        <managementContext>
            <managementContext createConnector="false"/>
        </managementContext>

        <!-- Shutdown hooks -->
        <shutdownHooks>
            <bean class="org.apache.activemq.hooks.SpringContextHook"/>
        </shutdownHooks>

    </broker>

    <!-- Web console -->
    <import resource="jetty.xml"/>

</beans>
```

---

## üîπ **Explanation of each tuning parameter**

### 1. **Destination Policy**

```xml
<policyEntry queue=">" producerFlowControl="false" memoryLimit="4gb">
    <pendingMessageLimitStrategy>
        <constantPendingMessageLimitStrategy limit="5000"/>
    </pendingMessageLimitStrategy>
</policyEntry>
```

* `queue=">"` ‚Üí applies to **all queues**.
* `producerFlowControl=false` ‚Üí producers are **never blocked by slow consumers**, improves throughput.
* `memoryLimit=4gb` ‚Üí how much memory broker uses per destination before throttling.
* `constantPendingMessageLimitStrategy limit="5000"` ‚Üí prevents a single consumer from hoarding messages.

---

### 2. **Persistence Adapter (KahaDB)**

```xml
<kahaDB journalMaxFileLength="128mb"
       enableIndexWriteAsync="true"
       enableJournalDiskSyncs="true"
       checkpointInterval="3000"/>
```

* `journalMaxFileLength` ‚Üí larger files = fewer rotations = better throughput.
* `enableIndexWriteAsync=true` ‚Üí async index writes improve speed.
* `enableJournalDiskSyncs=true` ‚Üí ensures **no data loss on crash**.
* `checkpointInterval=3000` ‚Üí write checkpoint every 3 seconds (balances durability & speed).

> ‚ö†Ô∏è If you can tolerate small data loss, you can set `enableJournalDiskSyncs=false` for higher throughput.

---

### 3. **System Usage**

```xml
<memoryUsage limit="16 gb"/>
<storeUsage limit="200 gb"/>
<tempUsage limit="100 gb"/>
```

* `memoryUsage` ‚Üí how much heap broker can use for in-flight messages.
* `storeUsage` ‚Üí max disk space for persistent messages.
* `tempUsage` ‚Üí temp storage for non-persistent messages.

> For millions/sec, default 1‚Äì2GB is too small ‚Üí increase as above.

---

### 4. **Transport Connectors**

```xml
<transportConnector name="openwire" uri="tcp://0.0.0.0:61616?maximumConnections=2000&amp;wireFormat.maxFrameSize=104857600"/>
```

* `maximumConnections=2000` ‚Üí allows many concurrent producer/consumer connections.
* `wireFormat.maxFrameSize=100MB` ‚Üí prevents network frame errors with large messages.
* You‚Äôve configured all protocols (openwire, AMQP, STOMP, MQTT, WebSocket) similarly.

---

### 5. **Scaling Tips**

* **Horizontal scaling**: run multiple brokers in a **network of brokers** if one broker cannot handle full load.
* **Consumers**: keep per-JVM concurrency \~50‚Äì100, scale horizontally with multiple JVMs.
* **Prefetch**: tune per consumer (e.g., 500‚Äì1000) to avoid memory pressure.

---

‚úÖ With this XML and your Spring Boot producer/consumer code:

* **No data loss** (persistent delivery + KahaDB fsync).
* **High throughput** (\~50k‚Äì200k msgs/sec per broker, can scale horizontally).
* **Stable memory usage** and disk limits.
* **Async send + async logging** ensures your JVM isn‚Äôt blocked.

---

Perfect! Let‚Äôs visualize how your **high-throughput ActiveMQ system** can scale with multiple JVMs and brokers, and estimate realistic **messages/sec**.

---

## üîπ **Architecture Diagram**

```
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ      ActiveMQ Broker 1      ‚îÇ
          ‚îÇ  (Classic, tuned XML)       ‚îÇ
          ‚îÇ  memory=16GB, store=200GB   ‚îÇ
          ‚îÇ  KahaDB persistent store    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚îÇ TCP connections
                        ‚îÇ (OpenWire, AMQP, MQTT...)
                        ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                            ‚îÇ
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ JVM App ‚îÇ                  ‚îÇ JVM App ‚îÇ
     ‚îÇ Producer‚îÇ                  ‚îÇ Consumer‚îÇ
     ‚îÇ  Concurrency=50‚Äì100         ‚îÇ Concurrency=50‚Äì100
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                            ‚îÇ
          ‚îÇ                            ‚îÇ
   Bulk send messages              Bulk read messages
          ‚îÇ                            ‚îÇ
          ‚îÇ                            ‚îÇ
          ‚ñº                            ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ JVM App 2     ‚îÇ            ‚îÇ JVM App N     ‚îÇ
   ‚îÇ Producer      ‚îÇ            ‚îÇ Consumer      ‚îÇ
   ‚îÇ Concurrency=50‚Äì100          ‚îÇ Concurrency=50‚Äì100
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

> ‚úÖ **Key points**:
>
> * Each JVM runs multiple consumers/producers (50‚Äì100).
> * Producers push bulk messages to a **queue** (not topic) ‚Üí messages are **load-balanced** among consumers.
> * Consumers in multiple JVMs can scale horizontally.
> * Broker handles persistence via KahaDB.

---

## üîπ **Throughput Estimation**

Assumptions:

* Small message size (\~1 KB).
* Persistent delivery enabled.
* Broker tuned as per XML I gave.
* Each consumer can handle \~5k msgs/sec (processing is minimal, logging async).

| Parameter                 | Value per JVM / Broker                                     |
| ------------------------- | ---------------------------------------------------------- |
| Consumers per JVM         | 50‚Äì100                                                     |
| Consumers per 10 JVMs     | 500‚Äì1000                                                   |
| Msg/sec per consumer      | \~5,000                                                    |
| Throughput per JVM        | 50‚Äì100 consumers √ó 5k = 250k‚Äì500k msgs/sec (theoretical)   |
| Throughput across 10 JVMs | 1M‚Äì5M msgs/sec (theoretical)                               |
| Broker disk limit         | 200 GB ‚Üí can hold 200M small messages (\~1KB each)         |
| Memory usage              | 16 GB ‚Üí enough for prefetch buffer (500‚Äì1000 per consumer) |

> ‚ö†Ô∏è Realistic sustained throughput: **\~1‚Äì2M msgs/sec across all JVMs**, because disk I/O and network overhead will reduce theoretical max.

---

## üîπ **Recommendations for Production**

1. **Multiple Brokers**:

   * For 1M+ msgs/sec, use a **network of 2‚Äì3 brokers**, split load with queues.
   * Use **store-and-forward** to balance load between brokers.

2. **Producer tuning**:

   * `jmsTemplate.setDeliveryPersistent(true)` for no message loss.
   * `setUseAsyncSend(true)` ‚Üí asynchronous network send.

3. **Consumer tuning**:

   * `DefaultJmsListenerContainerFactory` with concurrency 50‚Äì100 per JVM.
   * Prefetch = 500‚Äì1000 messages.

4. **Logging**:

   * Async logger (`logback AsyncAppender`) ‚Üí avoid System.out bottleneck.

5. **Monitoring**:

   * Enable JMX to watch `MemoryUsage`, `StoreUsage`, `QueueSize`, `Consumers`.
   * Add alerting if memory/store usage exceeds thresholds.

---

Yes ‚Äî with your **current setup**, your listener **can handle huge bulk messages**, but there are some important considerations and limits to be aware of:

---

### 1Ô∏è‚É£ **Why it can handle bulk**

* You are using **queues**, not topics ‚Üí messages are **load-balanced** across consumers.
* `DefaultJmsListenerContainerFactory` with `concurrency="50-100"` ‚Üí multiple parallel consumers in each JVM.
* **Async logging** ‚Üí logging doesn‚Äôt block message processing.
* Prefetch = 1000 ‚Üí consumers can prefetch multiple messages into memory to reduce network latency.
* Persistent delivery + KahaDB ‚Üí no messages lost even if broker restarts.

---

### 2Ô∏è‚É£ **Practical limits**

| Component                    | Practical Limit                                                |
| ---------------------------- | -------------------------------------------------------------- |
| Consumer concurrency per JVM | 50‚Äì100 recommended, more than 200 may saturate CPU/heap        |
| Prefetch                     | 500‚Äì1000 ‚Üí higher = more memory usage                          |
| Broker memory                | 16 GB ‚Üí enough for \~1M in-flight messages                     |
| Broker store                 | 200 GB ‚Üí enough for persistent messages (1 KB \~200M messages) |
| Network                      | 1‚Äì10 Gbps ‚Üí very large bulk may saturate network               |
| JVM heap                     | Make sure each JVM has 2‚Äì4 GB for message processing           |

> If you push millions of messages per second, one broker alone **cannot sustain it** ‚Äî need **network of brokers**.

---

### 3Ô∏è‚É£ **How to maximize listener throughput**

1. **Parallel consumers**: Multiple JVMs, each with 50‚Äì100 consumers.
2. **Async processing**: Don‚Äôt block `@JmsListener` thread ‚Äî delegate heavy processing to another thread pool.
3. **Batch processing**: If messages can be processed in batches (e.g., 100‚Äì1000), throughput increases dramatically.
4. **Tuning broker**: Your `activemq.xml` already increases memory and store usage.

---

‚úÖ **Summary:**

* Your listener **can handle huge bulk messages** (10k‚Äì100k/sec per JVM).
* To go beyond 0.5‚Äì1M messages/sec, you need **multiple JVMs + multiple brokers + async batch processing**.
* Persistent messages + KahaDB ensures **no data loss**.

---

Yes! Let‚Äôs estimate the **listener‚Äôs capacity** realistically based on your current setup. We‚Äôll break it down step by step.

---

## **1Ô∏è‚É£ Your Current Listener Setup**

* **Queue:** `gps.telemetry.queue` ‚Üí load-balanced among consumers.
* **Concurrency:** `50‚Äì100` consumers per JVM.
* **Prefetch:** 1000 messages per consumer.
* **Logging:** Async logging via `logback AsyncAppender`.
* **JMS:** Persistent messages, KahaDB store, ActiveMQ Classic 6.1.
* **Hardware assumptions:** 4‚Äì8 vCPU per JVM, 8‚Äì16 GB heap, broker memory 16 GB, broker disk 200 GB.

---

## **2Ô∏è‚É£ Throughput per consumer**

* Minimal processing (just logging) ‚Üí one consumer can handle **\~5,000‚Äì10,000 messages/sec** reliably.
* Heavy processing (parsing JSON, DB writes) ‚Üí throughput decreases.

---

## **3Ô∏è‚É£ Throughput per JVM**

| Consumers | Msg/sec per consumer | Total per JVM |
| --------- | -------------------- | ------------- |
| 50        | 5,000                | 250,000       |
| 100       | 5,000                | 500,000       |
| 50        | 10,000               | 500,000       |
| 100       | 10,000               | 1,000,000     |

> Realistic **continuous throughput**: \~250k‚Äì500k messages/sec per JVM with minimal processing.

---

## **4Ô∏è‚É£ Throughput across multiple JVMs**

| JVMs | Consumers per JVM | Total Consumers | Approx. Msg/sec |
| ---- | ----------------- | --------------- | --------------- |
| 5    | 50                | 250             | 1,250,000       |
| 10   | 100               | 1,000           | 5,000,000       |
| 10   | 50                | 500             | 2,500,000       |

> With **10 JVMs √ó 50‚Äì100 consumers**, you could theoretically handle **2‚Äì5 million messages/sec**.

---

## **5Ô∏è‚É£ Bottlenecks to watch**

1. **Broker memory:** 16 GB ‚Üí 1‚Äì2M in-flight messages. Increase if needed.
2. **Broker disk I/O:** Persistent messages ‚Üí KahaDB writes can be a bottleneck.
3. **Network:** Ensure sufficient bandwidth (1‚Äì10 Gbps).
4. **JVM heap:** Each JVM should have enough heap (2‚Äì4 GB) for prefetch buffers.
5. **Consumer processing:** Any heavy processing reduces effective throughput.

---

‚úÖ **Conclusion:**

* **Single JVM with 50‚Äì100 consumers** ‚Üí \~250k‚Äì1M messages/sec.
* **Multiple JVMs (10+)** ‚Üí 2‚Äì5M messages/sec possible.
* **Your listener code is capable** of handling **huge bulk messages**, provided broker memory, disk, and network are tuned.

---

If you want, I can **draw a chart showing exact listener capacity per JVM, consumers, and broker configuration**, so you can **plan scaling for 10M messages/sec**.

Do you want me to make that chart?

Perfect! Let‚Äôs make a **listener capacity chart** for your ActiveMQ setup so you can visualize **messages/sec per JVM and total throughput**.

---

## **Assumptions for the chart**

* Broker: ActiveMQ Classic 6.1, memory 16 GB, disk 200 GB (persistent KahaDB).
* Consumer processing: minimal (just logging).
* Prefetch: 1000 per consumer.
* JVM heap: 4 GB.
* Network: 10 Gbps.
* Concurrency: 50‚Äì100 consumers per JVM.
* Message size: \~1 KB.

---

## **1Ô∏è‚É£ Throughput per JVM**

| Consumers per JVM | Msg/sec per consumer | Total per JVM |
| ----------------- | -------------------- | ------------- |
| 50                | 5,000                | 250,000       |
| 100               | 5,000                | 500,000       |
| 50                | 10,000               | 500,000       |
| 100               | 10,000               | 1,000,000     |

> Notes:
>
> * The 10k/msg/sec per consumer is **optimistic**, achievable only with minimal processing and async logging.
> * Prefetch buffer and AsyncAppender reduce blocking.

---

## **2Ô∏è‚É£ Total throughput across JVMs**

| JVMs | Consumers per JVM | Total Consumers | Approx. Msg/sec |
| ---- | ----------------- | --------------- | --------------- |
| 1    | 50                | 50              | 250,000         |
| 1    | 100               | 100             | 500,000         |
| 5    | 50                | 250             | 1,250,000       |
| 5    | 100               | 500             | 2,500,000       |
| 10   | 50                | 500             | 2,500,000       |
| 10   | 100               | 1,000           | 5,000,000       |

> **Horizontal scaling:** Add more JVMs and consumers to increase throughput linearly until broker/network limits are hit.

---

## **3Ô∏è‚É£ Broker and system bottlenecks**

1. **Memory:** Keep in-flight messages < broker memory limit (16 GB ‚Üí \~16M 1KB messages).
2. **Disk (KahaDB):** Persistent messages ‚Üí heavy I/O. Consider SSDs for high throughput.
3. **Network:** 10 Gbps ‚Üí 1.25M 1KB messages/sec saturates \~1 Gbps. For millions/sec, you need **multi-gigabit network**.
4. **Consumer processing:** Avoid heavy synchronous processing; use **batching or thread pools**.

---

‚úÖ **Summary**

* **Single JVM, 50‚Äì100 consumers:** 250k‚Äì1M msgs/sec.
* **10 JVMs, 50‚Äì100 consumers:** 2.5M‚Äì5M msgs/sec.
* **Queue-based consumers** ‚Üí load-balanced, no duplicate messages.
* Async logging ‚Üí minimal processing overhead.
* Persistent messages ‚Üí no data loss.

---

  ![activemq](./activemq%20highthroughput%20.png)

---
